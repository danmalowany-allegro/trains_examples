{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-YsQrBjzNdX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (20.1.1)\n",
      "Requirement already up-to-date: torch==1.4.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already up-to-date: torchaudio==0.4.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: torch==1.4.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from torchaudio==0.4.0) (1.4.0)\n",
      "Requirement already up-to-date: matplotlib==3.2.1 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from matplotlib==3.2.1) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from matplotlib==3.2.1) (1.18.4)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from matplotlib==3.2.1) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from matplotlib==3.2.1) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from matplotlib==3.2.1) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib==3.2.1) (1.15.0)\n",
      "Requirement already up-to-date: trains==0.15.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (0.15.0)\n",
      "Requirement already satisfied, skipping upgrade: psutil>=3.4.2 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (5.7.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.3 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.19.5 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (4.46.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.10 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (1.18.4)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.16.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.11.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: pyjwt>=1.6.4 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: pathlib2>=2.3.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (2.3.5)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=4.1.1 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: typing>=3.6.4 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (3.7.4.1)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema>=2.6.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=18.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: jsonmodels>=2.2 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (2.4)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: furl>=2.0.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML>=3.12 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: funcsigs>=1.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: plotly>=3.9.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (4.8.1)\n",
      "Requirement already satisfied, skipping upgrade: requests-file>=1.4.2 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (1.5.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3>=1.21.1 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.20.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: humanfriendly>=2.1 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from trains==0.15.0) (8.2)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from jsonschema>=2.6.0->trains==0.15.0) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from jsonschema>=2.6.0->trains==0.15.0) (47.1.1)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from jsonschema>=2.6.0->trains==0.15.0) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: orderedmultidict>=1.0.1 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from furl>=2.0.0->trains==0.15.0) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from plotly>=3.9.0->trains==0.15.0) (1.3.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from requests>=2.20.0->trains==0.15.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from requests>=2.20.0->trains==0.15.0) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from requests>=2.20.0->trains==0.15.0) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=2.6.0->trains==0.15.0) (3.1.0)\n",
      "Requirement already up-to-date: pandas==1.0.4 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (1.0.4)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from pandas==1.0.4) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from pandas==1.0.4) (1.18.4)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from pandas==1.0.4) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.4) (1.15.0)\n",
      "Requirement already up-to-date: numpy==1.18.4 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (1.18.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorboard==2.2.1 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (2.2.1)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from tensorboard==2.2.1) (1.29.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from tensorboard==2.2.1) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from tensorboard==2.2.1) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from tensorboard==2.2.1) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from tensorboard==2.2.1) (1.18.4)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from tensorboard==2.2.1) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from tensorboard==2.2.1) (1.6.0.post3)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from tensorboard==2.2.1) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from tensorboard==2.2.1) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from tensorboard==2.2.1) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from tensorboard==2.2.1) (47.1.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from tensorboard==2.2.1) (3.12.2)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from tensorboard==2.2.1) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard==2.2.1) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.1) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.1) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.1) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.2.1) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.2.1) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.2.1) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.2.1) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.1) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard==2.2.1) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.2.1) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /home/sam/.pyenv/versions/3.7.4/envs/dan/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.1) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U pip\n",
    "! pip install -U torch==1.4.0\n",
    "! pip install -U torchaudio==0.4.0\n",
    "! pip install -U matplotlib==3.2.1\n",
    "! pip install -U trains==0.15.0\n",
    "! pip install -U pandas==1.0.4\n",
    "! pip install -U numpy==1.18.4\n",
    "! pip install -U tensorboard==2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T7T0Rf26zNdm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib2 import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "from trains import Task\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINS Task: overwriting (reusing) task id=c55aabc92e434015a2c97d26b7ac3578\n",
      "TRAINS results page: https://demoapp.trains.allegro.ai/projects/469622fa0a1842f9a90947bdc1773aea/experiments/c55aabc92e434015a2c97d26b7ac3578/output/log\n",
      "{'number_of_epochs': 10, 'batch_size': 4, 'dropout': 0.25, 'base_lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "task = Task.init(project_name='Audio Example', task_name='audio classifier')\n",
    "configuration_dict = {'number_of_epochs': 10, 'batch_size': 4, 'dropout': 0.25, 'base_lr': 0.001}\n",
    "configuration_dict = task.connect(configuration_dict)  # enabling configuration override by trains\n",
    "print(configuration_dict)  # printing actual configuration (after override in remote mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msiz7QdvzNeA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download UrbanSound8K dataset (https://urbansounddataset.weebly.com/urbansound8k.html)\n",
    "path_to_UrbanSound8K = '/home/sam/Datasets/UrbanSound8K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wXtmZe7yzNeS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 7895\n",
      "Test set size: 837\n"
     ]
    }
   ],
   "source": [
    "class UrbanSoundDataset(Dataset):\n",
    "#rapper for the UrbanSound8K dataset\n",
    "    def __init__(self, csv_path, file_path, folderList):\n",
    "        self.file_path = file_path\n",
    "        self.file_names = []\n",
    "        self.labels = []\n",
    "        self.folders = []\n",
    "        \n",
    "        #loop through the csv entries and only add entries from folders in the folder list\n",
    "        csvData = pd.read_csv(csv_path)\n",
    "        for i in range(0,len(csvData)):\n",
    "            if csvData.iloc[i, 5] in folderList:\n",
    "                self.file_names.append(csvData.iloc[i, 0])\n",
    "                self.labels.append(csvData.iloc[i, 6])\n",
    "                self.folders.append(csvData.iloc[i, 5])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #format the file path and load the file\n",
    "        path = self.file_path / (\"fold\" + str(self.folders[index])) / self.file_names[index]\n",
    "        sound, sample_rate = torchaudio.load(path, out = None, normalization = True)\n",
    "\n",
    "        # UrbanSound8K uses two channels, this will convert them to one\n",
    "        soundData = torch.mean(sound, dim=0, keepdim=True)\n",
    "        \n",
    "        #Make sure all files are the same size\n",
    "        if soundData.numel() < 160000:\n",
    "            fixedsize_data = torch.nn.functional.pad(soundData, (0, 160000 - soundData.numel()))\n",
    "        else:\n",
    "            fixedsize_data = soundData[0,:160000].reshape(1,160000)\n",
    "        \n",
    "        #downsample the audio\n",
    "        downsample_data = fixedsize_data[::5]\n",
    "        \n",
    "        melspectogram_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)\n",
    "        melspectogram = melspectogram_transform(downsample_data)\n",
    "        melspectogram_db = torchaudio.transforms.AmplitudeToDB()(melspectogram)\n",
    "\n",
    "        return fixedsize_data, sample_rate, melspectogram_db, self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "\n",
    "csv_path = Path(path_to_UrbanSound8K) / 'metadata' / 'UrbanSound8K.csv'\n",
    "file_path = Path(path_to_UrbanSound8K) / 'audio'\n",
    "\n",
    "train_set = UrbanSoundDataset(csv_path, file_path, range(1,10))\n",
    "test_set = UrbanSoundDataset(csv_path, file_path, [10])\n",
    "print(\"Train set size: \" + str(len(train_set)))\n",
    "print(\"Test set size: \" + str(len(test_set)))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = configuration_dict.get('batch_size', 4), \n",
    "                                           shuffle = True, pin_memory=True, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = configuration_dict.get('batch_size', 4), \n",
    "                                          shuffle = False, pin_memory=True, num_workers=1)\n",
    "\n",
    "classes = ('air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', 'engine_idling', \n",
    "           'gun_shot', 'jackhammer', 'siren', 'street_music')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylblw-k1zNeZ"
   },
   "outputs": [],
   "source": [
    "class CnnAudioNet(nn.Module):\n",
    "    def __init__(self,NumClasses):\n",
    "        super(CnnAudioNet,self).__init__()\n",
    "        self.NumClasses = NumClasses\n",
    "        self.Fc_features = 128\n",
    "        self.C1 = nn.Conv2d(1,16,3)\n",
    "        self.C2 = nn.Conv2d(16,32,3)\n",
    "        self.C3 = nn.Conv2d(32,64,3)\n",
    "        self.C4 = nn.Conv2d(64,128,3)\n",
    "        \n",
    "        self.BN1 = nn.BatchNorm2d(16)\n",
    "        self.BN2 = nn.BatchNorm2d(32)\n",
    "        self.BN3 = nn.BatchNorm2d(64)\n",
    "        self.BN4 = nn.BatchNorm2d(128)\n",
    "        self.maxpool1 = nn.MaxPool2d(2,2)        \n",
    "        \n",
    "        self.fc1 = nn.Linear(128*29*197,128)\n",
    "        self.fc2 = nn.Linear(128,self.NumClasses )\n",
    "        self.dropout = nn.Dropout(configuration_dict.get('dropout', 0.25))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = F.relu(self.C1(x))\n",
    "        x = self.maxpool1(F.relu(self.C2(x)))\n",
    "        x = F.relu(self.C3(x))\n",
    "        x = self.maxpool1(F.relu(self.C4(x)))\n",
    "        # flatten image input\n",
    "        x = x.view(-1,128*29*197)\n",
    "        x =  F.relu(self.fc1(self.dropout(x)))\n",
    "        x = self.fc2(self.dropout(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = CnnAudioNet(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3yKYru14zNef"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr = configuration_dict.get('base_lr', 0.001), momentum = 0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 3, gamma = 0.1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device to use: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CnnAudioNet(\n",
       "  (C1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (C2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (C3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (C4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (BN1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (BN2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (BN3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (BN4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=731264, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.cuda.current_device() if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Device to use: {}'.format(device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_writer = SummaryWriter('./tensorboard_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "def plot_signal(signal, title, cmap=None):\n",
    "    fig = plt.figure()\n",
    "    if signal.ndim == 1:\n",
    "        plt.plot(signal)\n",
    "    else:\n",
    "        plt.imshow(signal, cmap=cmap)    \n",
    "    plt.title(title)\n",
    "    \n",
    "    plot_buf = io.BytesIO()\n",
    "    plt.savefig(plot_buf, format='jpeg')\n",
    "    plot_buf.seek(0)\n",
    "    plt.close(fig)\n",
    "    return ToTensor()(PIL.Image.open(plot_buf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vdthqz3JzNem"
   },
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (sounds, sample_rate, inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iteration = epoch * len(train_loader) + batch_idx\n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'\n",
    "                  .format(epoch, batch_idx * len(inputs), len(train_loader.dataset), \n",
    "                          100. * batch_idx / len(train_loader), loss))\n",
    "            tensorboard_writer.add_scalar('training loss/loss', loss, iteration)\n",
    "            tensorboard_writer.add_scalar('learning rate/lr', optimizer.param_groups[0]['lr'], iteration)\n",
    "                \n",
    "        \n",
    "        if batch_idx % debug_interval == 0:    # report debug image every 500 mini-batches\n",
    "            for n, (inp, pred, label) in enumerate(zip(inputs, predicted, labels)):\n",
    "                series = 'label_{}_pred_{}'.format(classes[label.cpu()], classes[pred.cpu()])\n",
    "                tensorboard_writer.add_image('Train MelSpectrogram samples/{}'.format(n), \n",
    "                                             plot_signal(inp.cpu().numpy().squeeze(), series, 'hot'), iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LBWoj7u5zNes"
   },
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        for idx, (sounds, sample_rate, inputs, labels) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels)\n",
    "            for i in range(len(inputs)):\n",
    "                label = labels[i].item()\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "        \n",
    "            iteration = (epoch + 1) * len(train_loader)\n",
    "            if idx % debug_interval == 0:    # report debug image every 100 mini-batches\n",
    "                for n, (sound, inp, pred, label) in enumerate(zip(sounds, inputs, predicted, labels)):\n",
    "                    series = 'label_{}_pred_{}'.format(classes[label.cpu()], classes[pred.cpu()])\n",
    "                    print(iteration, sample_rate)\n",
    "                    tensorboard_writer.add_audio('Test audio samples/{}'.format(n), \n",
    "                                                 sound, iteration, int(sample_rate[n]))\n",
    "                    tensorboard_writer.add_image('Test MelSpectrogram samples/{}_{}'.format(idx, n), \n",
    "                                                 plot_signal(inp.cpu().numpy().squeeze(), series, 'hot'), iteration)\n",
    "\n",
    "    total_accuracy = 100 * sum(class_correct)/sum(class_total)\n",
    "    print('[Iteration {}] Accuracy on the {} test images: {}%\\n'.format(epoch, sum(class_total), total_accuracy))\n",
    "    tensorboard_writer.add_scalar('accuracy/total', total_accuracy, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5lx3g_5zNey",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/7895 (0%)]\tLoss: 2.314560\n",
      "Train Epoch: 0 [400/7895 (5%)]\tLoss: 2.281619\n",
      "Train Epoch: 0 [800/7895 (10%)]\tLoss: 2.479096\n",
      "Train Epoch: 0 [1200/7895 (15%)]\tLoss: 2.050175\n",
      "Train Epoch: 0 [1600/7895 (20%)]\tLoss: 1.865820\n",
      "Train Epoch: 0 [2000/7895 (25%)]\tLoss: 2.286825\n",
      "Train Epoch: 0 [2400/7895 (30%)]\tLoss: 2.025537\n",
      "Train Epoch: 0 [2800/7895 (35%)]\tLoss: 1.976062\n",
      "Train Epoch: 0 [3200/7895 (41%)]\tLoss: 2.277118\n",
      "Train Epoch: 0 [3600/7895 (46%)]\tLoss: 1.865269\n",
      "Train Epoch: 0 [4000/7895 (51%)]\tLoss: 1.828044\n",
      "Train Epoch: 0 [4400/7895 (56%)]\tLoss: 1.754791\n",
      "Train Epoch: 0 [4800/7895 (61%)]\tLoss: 1.316154\n",
      "Train Epoch: 0 [5200/7895 (66%)]\tLoss: 3.064564\n",
      "Train Epoch: 0 [5600/7895 (71%)]\tLoss: 1.080945\n",
      "Train Epoch: 0 [6000/7895 (76%)]\tLoss: 1.787307\n",
      "Train Epoch: 0 [6400/7895 (81%)]\tLoss: 1.813057\n",
      "Train Epoch: 0 [6800/7895 (86%)]\tLoss: 3.648711\n",
      "Train Epoch: 0 [7200/7895 (91%)]\tLoss: 2.033821\n",
      "Train Epoch: 0 [7600/7895 (96%)]\tLoss: 1.137187\n",
      "1974 tensor([44100, 44100, 44100, 44100])\n",
      "1974 tensor([44100, 44100, 44100, 44100])\n",
      "1974 tensor([44100, 44100, 44100, 44100])\n",
      "1974 tensor([44100, 44100, 44100, 44100])\n",
      "1974 tensor([44100, 44100, 44100, 44100])\n",
      "1974 tensor([44100, 44100, 44100, 44100])\n",
      "1974 tensor([44100, 44100, 44100, 44100])\n",
      "1974 tensor([44100, 44100, 44100, 44100])\n",
      "[Iteration 0] Accuracy on the 837.0 test images: 35.24492234169654%\n",
      "\n",
      "Train Epoch: 1 [0/7895 (0%)]\tLoss: 2.052357\n",
      "Train Epoch: 1 [400/7895 (5%)]\tLoss: 1.586272\n",
      "Train Epoch: 1 [800/7895 (10%)]\tLoss: 1.452763\n",
      "Train Epoch: 1 [1200/7895 (15%)]\tLoss: 0.912510\n",
      "Train Epoch: 1 [1600/7895 (20%)]\tLoss: 1.455922\n",
      "Train Epoch: 1 [2000/7895 (25%)]\tLoss: 2.303067\n",
      "Train Epoch: 1 [2400/7895 (30%)]\tLoss: 1.348102\n",
      "Train Epoch: 1 [2800/7895 (35%)]\tLoss: 1.541396\n",
      "Train Epoch: 1 [3200/7895 (41%)]\tLoss: 1.307455\n",
      "Train Epoch: 1 [3600/7895 (46%)]\tLoss: 1.601212\n",
      "Train Epoch: 1 [4000/7895 (51%)]\tLoss: 2.099861\n",
      "Train Epoch: 1 [4400/7895 (56%)]\tLoss: 1.508544\n",
      "Train Epoch: 1 [4800/7895 (61%)]\tLoss: 1.427874\n",
      "Train Epoch: 1 [5200/7895 (66%)]\tLoss: 1.372240\n",
      "Train Epoch: 1 [5600/7895 (71%)]\tLoss: 1.504646\n",
      "Train Epoch: 1 [6000/7895 (76%)]\tLoss: 0.808984\n",
      "Train Epoch: 1 [6400/7895 (81%)]\tLoss: 0.757257\n",
      "Train Epoch: 1 [6800/7895 (86%)]\tLoss: 0.963127\n",
      "Train Epoch: 1 [7200/7895 (91%)]\tLoss: 2.083896\n",
      "Train Epoch: 1 [7600/7895 (96%)]\tLoss: 1.542810\n",
      "3948 tensor([44100, 44100, 44100, 44100])\n",
      "3948 tensor([44100, 44100, 44100, 44100])\n",
      "3948 tensor([44100, 44100, 44100, 44100])\n",
      "3948 tensor([44100, 44100, 44100, 44100])\n",
      "3948 tensor([44100, 44100, 44100, 44100])\n",
      "3948 tensor([44100, 44100, 44100, 44100])\n",
      "3948 tensor([44100, 44100, 44100, 44100])\n",
      "3948 tensor([44100, 44100, 44100, 44100])\n",
      "[Iteration 1] Accuracy on the 837.0 test images: 55.794504181600956%\n",
      "\n",
      "Train Epoch: 2 [0/7895 (0%)]\tLoss: 1.637645\n",
      "Train Epoch: 2 [400/7895 (5%)]\tLoss: 0.394647\n",
      "Train Epoch: 2 [800/7895 (10%)]\tLoss: 0.762153\n",
      "Train Epoch: 2 [1200/7895 (15%)]\tLoss: 1.764579\n",
      "Train Epoch: 2 [1600/7895 (20%)]\tLoss: 0.526774\n",
      "Train Epoch: 2 [2000/7895 (25%)]\tLoss: 1.324988\n",
      "Train Epoch: 2 [2400/7895 (30%)]\tLoss: 2.876887\n",
      "Train Epoch: 2 [2800/7895 (35%)]\tLoss: 0.958580\n",
      "Train Epoch: 2 [3200/7895 (41%)]\tLoss: 1.016143\n",
      "Train Epoch: 2 [3600/7895 (46%)]\tLoss: 1.701749\n",
      "Train Epoch: 2 [4000/7895 (51%)]\tLoss: 1.188366\n",
      "Train Epoch: 2 [4400/7895 (56%)]\tLoss: 1.022829\n",
      "Train Epoch: 2 [4800/7895 (61%)]\tLoss: 1.001718\n",
      "Train Epoch: 2 [5200/7895 (66%)]\tLoss: 1.611018\n",
      "Train Epoch: 2 [5600/7895 (71%)]\tLoss: 1.420505\n",
      "Train Epoch: 2 [6000/7895 (76%)]\tLoss: 2.150860\n",
      "Train Epoch: 2 [6400/7895 (81%)]\tLoss: 0.666773\n",
      "Train Epoch: 2 [6800/7895 (86%)]\tLoss: 1.170002\n",
      "Train Epoch: 2 [7200/7895 (91%)]\tLoss: 0.651466\n",
      "Train Epoch: 2 [7600/7895 (96%)]\tLoss: 1.387118\n",
      "5922 tensor([44100, 44100, 44100, 44100])\n",
      "5922 tensor([44100, 44100, 44100, 44100])\n",
      "5922 tensor([44100, 44100, 44100, 44100])\n",
      "5922 tensor([44100, 44100, 44100, 44100])\n",
      "5922 tensor([44100, 44100, 44100, 44100])\n",
      "5922 tensor([44100, 44100, 44100, 44100])\n",
      "5922 tensor([44100, 44100, 44100, 44100])\n",
      "5922 tensor([44100, 44100, 44100, 44100])\n",
      "[Iteration 2] Accuracy on the 837.0 test images: 48.028673835125446%\n",
      "\n",
      "Train Epoch: 3 [0/7895 (0%)]\tLoss: 0.406957\n",
      "Train Epoch: 3 [400/7895 (5%)]\tLoss: 1.292611\n",
      "Train Epoch: 3 [800/7895 (10%)]\tLoss: 0.563824\n",
      "Train Epoch: 3 [1200/7895 (15%)]\tLoss: 0.583851\n",
      "Train Epoch: 3 [1600/7895 (20%)]\tLoss: 0.260955\n",
      "Train Epoch: 3 [2000/7895 (25%)]\tLoss: 0.825215\n",
      "Train Epoch: 3 [2400/7895 (30%)]\tLoss: 0.241350\n",
      "Train Epoch: 3 [2800/7895 (35%)]\tLoss: 0.895427\n",
      "Train Epoch: 3 [3200/7895 (41%)]\tLoss: 0.665902\n",
      "Train Epoch: 3 [3600/7895 (46%)]\tLoss: 0.995529\n",
      "Train Epoch: 3 [4000/7895 (51%)]\tLoss: 1.320227\n",
      "Train Epoch: 3 [4400/7895 (56%)]\tLoss: 1.163547\n",
      "Train Epoch: 3 [4800/7895 (61%)]\tLoss: 0.678849\n",
      "Train Epoch: 3 [5200/7895 (66%)]\tLoss: 1.436893\n",
      "Train Epoch: 3 [5600/7895 (71%)]\tLoss: 0.660603\n",
      "Train Epoch: 3 [6000/7895 (76%)]\tLoss: 0.704131\n",
      "Train Epoch: 3 [6400/7895 (81%)]\tLoss: 0.669364\n",
      "Train Epoch: 3 [6800/7895 (86%)]\tLoss: 0.973579\n",
      "Train Epoch: 3 [7200/7895 (91%)]\tLoss: 0.734542\n",
      "Train Epoch: 3 [7600/7895 (96%)]\tLoss: 1.206385\n",
      "7896 tensor([44100, 44100, 44100, 44100])\n",
      "7896 tensor([44100, 44100, 44100, 44100])\n",
      "7896 tensor([44100, 44100, 44100, 44100])\n",
      "7896 tensor([44100, 44100, 44100, 44100])\n",
      "7896 tensor([44100, 44100, 44100, 44100])\n",
      "7896 tensor([44100, 44100, 44100, 44100])\n",
      "7896 tensor([44100, 44100, 44100, 44100])\n",
      "7896 tensor([44100, 44100, 44100, 44100])\n",
      "[Iteration 3] Accuracy on the 837.0 test images: 54.48028673835125%\n",
      "\n",
      "Train Epoch: 4 [0/7895 (0%)]\tLoss: 0.430275\n",
      "Train Epoch: 4 [400/7895 (5%)]\tLoss: 3.100470\n",
      "Train Epoch: 4 [800/7895 (10%)]\tLoss: 0.535463\n",
      "Train Epoch: 4 [1200/7895 (15%)]\tLoss: 0.044415\n",
      "Train Epoch: 4 [1600/7895 (20%)]\tLoss: 0.660827\n",
      "Train Epoch: 4 [2000/7895 (25%)]\tLoss: 0.404990\n",
      "Train Epoch: 4 [2400/7895 (30%)]\tLoss: 0.229916\n",
      "Train Epoch: 4 [2800/7895 (35%)]\tLoss: 0.132405\n",
      "Train Epoch: 4 [3200/7895 (41%)]\tLoss: 0.101067\n",
      "Train Epoch: 4 [3600/7895 (46%)]\tLoss: 0.434039\n",
      "Train Epoch: 4 [4000/7895 (51%)]\tLoss: 0.711258\n",
      "Train Epoch: 4 [4400/7895 (56%)]\tLoss: 0.337513\n",
      "Train Epoch: 4 [4800/7895 (61%)]\tLoss: 1.491033\n",
      "Train Epoch: 4 [5200/7895 (66%)]\tLoss: 1.332004\n",
      "Train Epoch: 4 [5600/7895 (71%)]\tLoss: 0.462150\n",
      "Train Epoch: 4 [6000/7895 (76%)]\tLoss: 0.238873\n",
      "Train Epoch: 4 [6400/7895 (81%)]\tLoss: 0.350446\n",
      "Train Epoch: 4 [6800/7895 (86%)]\tLoss: 1.092299\n",
      "Train Epoch: 4 [7200/7895 (91%)]\tLoss: 1.150412\n",
      "Train Epoch: 4 [7600/7895 (96%)]\tLoss: 0.483001\n",
      "9870 tensor([44100, 44100, 44100, 44100])\n",
      "9870 tensor([44100, 44100, 44100, 44100])\n",
      "9870 tensor([44100, 44100, 44100, 44100])\n",
      "9870 tensor([44100, 44100, 44100, 44100])\n",
      "9870 tensor([44100, 44100, 44100, 44100])\n",
      "9870 tensor([44100, 44100, 44100, 44100])\n",
      "9870 tensor([44100, 44100, 44100, 44100])\n",
      "9870 tensor([44100, 44100, 44100, 44100])\n",
      "[Iteration 4] Accuracy on the 837.0 test images: 57.94504181600956%\n",
      "\n",
      "Train Epoch: 5 [0/7895 (0%)]\tLoss: 1.472514\n",
      "Train Epoch: 5 [400/7895 (5%)]\tLoss: 0.537343\n",
      "Train Epoch: 5 [800/7895 (10%)]\tLoss: 0.055026\n",
      "Train Epoch: 5 [1200/7895 (15%)]\tLoss: 0.138967\n",
      "Train Epoch: 5 [1600/7895 (20%)]\tLoss: 1.033029\n",
      "Train Epoch: 5 [2000/7895 (25%)]\tLoss: 0.429652\n",
      "Train Epoch: 5 [2400/7895 (30%)]\tLoss: 0.817153\n",
      "Train Epoch: 5 [2800/7895 (35%)]\tLoss: 0.045780\n",
      "Train Epoch: 5 [3200/7895 (41%)]\tLoss: 0.394254\n",
      "Train Epoch: 5 [3600/7895 (46%)]\tLoss: 0.351882\n",
      "Train Epoch: 5 [4000/7895 (51%)]\tLoss: 0.750896\n",
      "Train Epoch: 5 [4400/7895 (56%)]\tLoss: 0.011727\n",
      "Train Epoch: 5 [4800/7895 (61%)]\tLoss: 0.746249\n",
      "Train Epoch: 5 [5200/7895 (66%)]\tLoss: 0.326156\n",
      "Train Epoch: 5 [5600/7895 (71%)]\tLoss: 0.930185\n",
      "Train Epoch: 5 [6000/7895 (76%)]\tLoss: 0.232350\n",
      "Train Epoch: 5 [6400/7895 (81%)]\tLoss: 0.466667\n",
      "Train Epoch: 5 [6800/7895 (86%)]\tLoss: 0.367227\n",
      "Train Epoch: 5 [7200/7895 (91%)]\tLoss: 0.304088\n",
      "Train Epoch: 5 [7600/7895 (96%)]\tLoss: 0.562914\n",
      "11844 tensor([44100, 44100, 44100, 44100])\n",
      "11844 tensor([44100, 44100, 44100, 44100])\n",
      "11844 tensor([44100, 44100, 44100, 44100])\n",
      "11844 tensor([44100, 44100, 44100, 44100])\n",
      "11844 tensor([44100, 44100, 44100, 44100])\n",
      "11844 tensor([44100, 44100, 44100, 44100])\n",
      "11844 tensor([44100, 44100, 44100, 44100])\n",
      "11844 tensor([44100, 44100, 44100, 44100])\n",
      "[Iteration 5] Accuracy on the 837.0 test images: 58.303464755077655%\n",
      "\n",
      "Train Epoch: 6 [0/7895 (0%)]\tLoss: 0.511781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [400/7895 (5%)]\tLoss: 0.160859\n",
      "Train Epoch: 6 [800/7895 (10%)]\tLoss: 0.664670\n",
      "Train Epoch: 6 [1200/7895 (15%)]\tLoss: 0.890169\n",
      "Train Epoch: 6 [1600/7895 (20%)]\tLoss: 0.584988\n",
      "Train Epoch: 6 [2000/7895 (25%)]\tLoss: 1.022784\n",
      "Train Epoch: 6 [2400/7895 (30%)]\tLoss: 0.345562\n",
      "Train Epoch: 6 [2800/7895 (35%)]\tLoss: 0.083229\n",
      "Train Epoch: 6 [3200/7895 (41%)]\tLoss: 0.281698\n",
      "Train Epoch: 6 [3600/7895 (46%)]\tLoss: 0.542019\n",
      "Train Epoch: 6 [4000/7895 (51%)]\tLoss: 0.412350\n",
      "Train Epoch: 6 [4400/7895 (56%)]\tLoss: 0.608566\n",
      "Train Epoch: 6 [4800/7895 (61%)]\tLoss: 0.855969\n",
      "Train Epoch: 6 [5200/7895 (66%)]\tLoss: 0.060554\n",
      "Train Epoch: 6 [5600/7895 (71%)]\tLoss: 0.490298\n",
      "Train Epoch: 6 [6000/7895 (76%)]\tLoss: 0.274956\n",
      "Train Epoch: 6 [6400/7895 (81%)]\tLoss: 0.688784\n",
      "Train Epoch: 6 [6800/7895 (86%)]\tLoss: 1.309547\n",
      "Train Epoch: 6 [7200/7895 (91%)]\tLoss: 0.731241\n",
      "Train Epoch: 6 [7600/7895 (96%)]\tLoss: 0.164598\n",
      "13818 tensor([44100, 44100, 44100, 44100])\n",
      "13818 tensor([44100, 44100, 44100, 44100])\n",
      "13818 tensor([44100, 44100, 44100, 44100])\n",
      "13818 tensor([44100, 44100, 44100, 44100])\n",
      "13818 tensor([44100, 44100, 44100, 44100])\n",
      "13818 tensor([44100, 44100, 44100, 44100])\n",
      "13818 tensor([44100, 44100, 44100, 44100])\n",
      "13818 tensor([44100, 44100, 44100, 44100])\n",
      "[Iteration 6] Accuracy on the 837.0 test images: 59.02031063321386%\n",
      "\n",
      "Train Epoch: 7 [0/7895 (0%)]\tLoss: 0.117742\n",
      "Train Epoch: 7 [400/7895 (5%)]\tLoss: 1.005423\n",
      "Train Epoch: 7 [800/7895 (10%)]\tLoss: 0.518922\n",
      "Train Epoch: 7 [1200/7895 (15%)]\tLoss: 0.312900\n",
      "Train Epoch: 7 [1600/7895 (20%)]\tLoss: 0.131205\n",
      "Train Epoch: 7 [2000/7895 (25%)]\tLoss: 0.886069\n",
      "Train Epoch: 7 [2400/7895 (30%)]\tLoss: 0.015992\n",
      "Train Epoch: 7 [2800/7895 (35%)]\tLoss: 0.030708\n",
      "Train Epoch: 7 [3200/7895 (41%)]\tLoss: 0.618057\n",
      "Train Epoch: 7 [3600/7895 (46%)]\tLoss: 0.069695\n",
      "Train Epoch: 7 [4000/7895 (51%)]\tLoss: 0.065017\n",
      "Train Epoch: 7 [4400/7895 (56%)]\tLoss: 1.117380\n",
      "Train Epoch: 7 [4800/7895 (61%)]\tLoss: 0.925279\n",
      "Train Epoch: 7 [5200/7895 (66%)]\tLoss: 0.219090\n",
      "Train Epoch: 7 [5600/7895 (71%)]\tLoss: 0.488855\n",
      "Train Epoch: 7 [6000/7895 (76%)]\tLoss: 0.465757\n",
      "Train Epoch: 7 [6400/7895 (81%)]\tLoss: 0.004715\n",
      "Train Epoch: 7 [6800/7895 (86%)]\tLoss: 0.601502\n",
      "Train Epoch: 7 [7200/7895 (91%)]\tLoss: 0.134972\n",
      "Train Epoch: 7 [7600/7895 (96%)]\tLoss: 0.859325\n",
      "15792 tensor([44100, 44100, 44100, 44100])\n",
      "15792 tensor([44100, 44100, 44100, 44100])\n",
      "15792 tensor([44100, 44100, 44100, 44100])\n",
      "15792 tensor([44100, 44100, 44100, 44100])\n",
      "15792 tensor([44100, 44100, 44100, 44100])\n",
      "15792 tensor([44100, 44100, 44100, 44100])\n",
      "15792 tensor([44100, 44100, 44100, 44100])\n",
      "15792 tensor([44100, 44100, 44100, 44100])\n",
      "[Iteration 7] Accuracy on the 837.0 test images: 58.183990442054956%\n",
      "\n",
      "Train Epoch: 8 [0/7895 (0%)]\tLoss: 0.003130\n",
      "Train Epoch: 8 [400/7895 (5%)]\tLoss: 0.566107\n",
      "Train Epoch: 8 [800/7895 (10%)]\tLoss: 1.163389\n",
      "Train Epoch: 8 [1200/7895 (15%)]\tLoss: 0.198834\n",
      "Train Epoch: 8 [1600/7895 (20%)]\tLoss: 0.056532\n",
      "Train Epoch: 8 [2000/7895 (25%)]\tLoss: 0.777416\n",
      "Train Epoch: 8 [2400/7895 (30%)]\tLoss: 0.086221\n",
      "Train Epoch: 8 [2800/7895 (35%)]\tLoss: 0.061117\n",
      "Train Epoch: 8 [3200/7895 (41%)]\tLoss: 0.150267\n",
      "Train Epoch: 8 [3600/7895 (46%)]\tLoss: 0.797577\n",
      "Train Epoch: 8 [4000/7895 (51%)]\tLoss: 0.022759\n",
      "Train Epoch: 8 [4400/7895 (56%)]\tLoss: 0.026345\n",
      "Train Epoch: 8 [4800/7895 (61%)]\tLoss: 0.501297\n",
      "Train Epoch: 8 [5200/7895 (66%)]\tLoss: 0.048653\n",
      "Train Epoch: 8 [5600/7895 (71%)]\tLoss: 0.258350\n",
      "Train Epoch: 8 [6000/7895 (76%)]\tLoss: 0.021739\n",
      "Train Epoch: 8 [6400/7895 (81%)]\tLoss: 0.408313\n",
      "Train Epoch: 8 [6800/7895 (86%)]\tLoss: 0.002948\n",
      "Train Epoch: 8 [7200/7895 (91%)]\tLoss: 0.594403\n",
      "Train Epoch: 8 [7600/7895 (96%)]\tLoss: 0.000656\n",
      "17766 tensor([44100, 44100, 44100, 44100])\n",
      "17766 tensor([44100, 44100, 44100, 44100])\n",
      "17766 tensor([44100, 44100, 44100, 44100])\n",
      "17766 tensor([44100, 44100, 44100, 44100])\n",
      "17766 tensor([44100, 44100, 44100, 44100])\n",
      "17766 tensor([44100, 44100, 44100, 44100])\n",
      "17766 tensor([44100, 44100, 44100, 44100])\n",
      "17766 tensor([44100, 44100, 44100, 44100])\n",
      "[Iteration 8] Accuracy on the 837.0 test images: 59.61768219832736%\n",
      "\n",
      "Train Epoch: 9 [0/7895 (0%)]\tLoss: 0.068590\n",
      "Train Epoch: 9 [400/7895 (5%)]\tLoss: 0.035486\n",
      "Train Epoch: 9 [800/7895 (10%)]\tLoss: 0.667516\n",
      "Train Epoch: 9 [1200/7895 (15%)]\tLoss: 0.331849\n",
      "Train Epoch: 9 [1600/7895 (20%)]\tLoss: 0.129588\n",
      "Train Epoch: 9 [2000/7895 (25%)]\tLoss: 0.325682\n",
      "Train Epoch: 9 [2400/7895 (30%)]\tLoss: 0.153186\n",
      "Train Epoch: 9 [2800/7895 (35%)]\tLoss: 0.472789\n",
      "Train Epoch: 9 [3200/7895 (41%)]\tLoss: 1.661939\n",
      "Train Epoch: 9 [3600/7895 (46%)]\tLoss: 1.630794\n",
      "Train Epoch: 9 [4000/7895 (51%)]\tLoss: 0.632205\n",
      "Train Epoch: 9 [4400/7895 (56%)]\tLoss: 0.915804\n",
      "Train Epoch: 9 [4800/7895 (61%)]\tLoss: 0.022117\n",
      "Train Epoch: 9 [5200/7895 (66%)]\tLoss: 0.401929\n",
      "Train Epoch: 9 [5600/7895 (71%)]\tLoss: 0.011082\n",
      "Train Epoch: 9 [6000/7895 (76%)]\tLoss: 0.379599\n",
      "Train Epoch: 9 [6400/7895 (81%)]\tLoss: 0.777296\n",
      "Train Epoch: 9 [6800/7895 (86%)]\tLoss: 0.132929\n",
      "Train Epoch: 9 [7200/7895 (91%)]\tLoss: 0.021067\n",
      "Train Epoch: 9 [7600/7895 (96%)]\tLoss: 0.292313\n",
      "19740 tensor([44100, 44100, 44100, 44100])\n",
      "19740 tensor([44100, 44100, 44100, 44100])\n",
      "19740 tensor([44100, 44100, 44100, 44100])\n",
      "19740 tensor([44100, 44100, 44100, 44100])\n",
      "19740 tensor([44100, 44100, 44100, 44100])\n",
      "19740 tensor([44100, 44100, 44100, 44100])\n",
      "19740 tensor([44100, 44100, 44100, 44100])\n",
      "19740 tensor([44100, 44100, 44100, 44100])\n",
      "[Iteration 9] Accuracy on the 837.0 test images: 58.54241338112306%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_interval = 100\n",
    "debug_interval = 200\n",
    "for epoch in range(configuration_dict.get('number_of_epochs', 10)):\n",
    "    train(model, epoch)\n",
    "    test(model, epoch)\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "audio_classifier_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
